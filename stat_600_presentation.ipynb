{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stat-600-presentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DylanLoader/Statistics-600-seminar/blob/master/stat_600_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTLZDP16S7pK",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification with Google Colab\n",
        "## Author: Dylan Loader\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## What is Google Colaboratory?\n",
        "\n",
        "- A free google based notebook with R, Python, and Markdown support\n",
        "- Google Drive Integration\n",
        "- Comes with a large number of pre-installed packages\n",
        "- Access to Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs)\n",
        "- Easy error handling with Stackoverflow integration\n",
        "- Connectivity with cloud resources such as Google Cloud\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "musU69zHihBN",
        "colab_type": "text"
      },
      "source": [
        "# The MNIST Handwritten Dataset\n",
        "\n",
        "---\n",
        "\n",
        "- The MNIST dataset consists of 70,000 black and white images of handwritten digits provided by Dr. Yann LeCun\n",
        "- The images are available as 28x28 pixel images (usually represented as 28x28x1 vector)\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "Source: https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\n",
        "\n",
        "![](https://koenig-media.raywenderlich.com/uploads/2018/01/firstXsample.png)\n",
        "Source: https://koenig-media.raywenderlich.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_NWf_9CS3Ft",
        "colab_type": "text"
      },
      "source": [
        "# A 5-10 minute Crash Course in Convolutional Neural Nets (CNNs)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Motivation for CNNs over NN\n",
        "\n",
        "- Good for sequentially related data (images/audio/text)\n",
        "- Ease of computing relative to large Neural Networks\n",
        "- Frequently used in Computer Vision\n",
        "- Generalize better to data that is distrorted such as rotations and blur\n",
        "\n",
        "\n",
        "\n",
        "#### Ex. Labradoodle vs. Fried Chicken\n",
        "\n",
        "![](https://pbs.twimg.com/media/DCIdebqW0AAIxm6?format=jpg&name=small)\n",
        "\n",
        "Source: https://twitter.com/teenybiscuit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aQdz5kBoNpc",
        "colab_type": "text"
      },
      "source": [
        "## CNN Layers\n",
        "---\n",
        "\n",
        "### Example Diagram\n",
        "![An Example CNN](https://codetolight.files.wordpress.com/2017/11/network.png?w=1108)\n",
        "\n",
        "Source: https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/\n",
        "\n",
        "\n",
        "\n",
        "### Convolution Layers\n",
        "\n",
        "- The convolution operation in a CNN is similar to the mathematical convolution of two functions to create a third function.\n",
        "\n",
        "\n",
        "\n",
        "- In the case of our image recognition we use a filter(blue) generated from a uniform distribution centered at 0. The purple image is the feature map. \n",
        "\n",
        "\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/convolution-layer-a.png)\n",
        "\n",
        "Source: http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/\n",
        "\n",
        "### Activation Functions\n",
        "\n",
        "After each convolution an activation function is applied to elements of the matrix for two reasons:\n",
        "\n",
        "1. Introduce non-linearity into our model\n",
        "\n",
        "2. Improve computation time due the removal of negative floating point numbers\n",
        "\n",
        "Ex. \n",
        "\n",
        "![](https://www.embedded-vision.com/sites/default/files/technical-articles/CadenceCNN/Figure8.jpg)\n",
        "\n",
        "Source: https://www.embedded-vision.com/platinum-members/cadence/embedded-vision-training/documents/pages/neuralnetworksimagerecognition\n",
        "\n",
        "### Pooling Layers\n",
        "\n",
        "Pooling layers have two main benefits:\n",
        "\n",
        "1. Reduce computation time by reducing size of layers in the network.\n",
        "\n",
        "2. Preserves the detected features.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/max-pooling-a.png)\n",
        "\n",
        "\n",
        "### Drop-out Layers\n",
        "\n",
        "- Commonly used in fully connected networks to prevent overfitting and to improve computation time.\n",
        "\n",
        "- Often replaced by weight decay in Deep Networks.\n",
        "\n",
        "### Fully Connected Layers\n",
        "\n",
        "- After convolution and pooling layers the current weights are flattened and passed through a fully connected layer.\n",
        "\n",
        "- Usually after the fully connected layers we apply a high rate of dropout (in this case 50%) and pass the remaining values to an output activation function.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/fully-connected.png)\n",
        "\n",
        "\n",
        "\n",
        "Source: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#layer\n",
        "\n",
        "### Output Layer (Softmax)\n",
        "\n",
        "- One of the most common output activation functions is softmax. \n",
        "- It is approximately a generalization of the logistic classifier\n",
        "\n",
        "Let $x \\in \\mathbb{R}^k$. The softmax function is defined as: \n",
        "\n",
        "$$\\vec{p}=\\begin{pmatrix} p_1\\\\ .\\\\ .\\\\ .\\\\ p_k\\end{pmatrix};\n",
        "p_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHQwcuCvuKxo",
        "colab_type": "text"
      },
      "source": [
        "# CNN Training In Browser\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suGybNyutUGa",
        "colab_type": "text"
      },
      "source": [
        "## Google Colab has Custom Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRX3LEg4D-3F",
        "colab_type": "code",
        "outputId": "9eb58cb9-eba6-4d29-f8bc-56d293c49dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# Install non-native packages\n",
        "!pip install keras-tqdm\n",
        "!pip install pillow"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tqdm in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (2.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (4.28.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.16.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.12.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upDN5FQZb-CB",
        "colab_type": "text"
      },
      "source": [
        "## Google Colab has Several Pre-installed Packages\n",
        "\n",
        "Here we import Keras, SKlearn, and TensorFlow all without having to go through all the time consuming local installation. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSnLvFlARexO",
        "colab_type": "code",
        "outputId": "cf9c5cb5-9322-4186-8d9b-988fbff9e092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Source of original: https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6\n",
        "# This notebook is modified to run by fetching data from openml instead of csv\n",
        "\n",
        "# import packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from PIL import Image\n",
        "import glob\n",
        "from numpy.random import seed\n",
        "seed(1775)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, MaxPool2D\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "print(\"Tensorflow version \" + tf.__version__)\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K8JHlecUPtw",
        "colab_type": "text"
      },
      "source": [
        "## Verify That We're Using Our TPU or GPU\n",
        "\n",
        "![]('../hosted-runtime.PNG')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n-k9qlEUGYw",
        "colab_type": "code",
        "outputId": "8cb2f9e0-aa0e-4bc9-c9db-00e6050a9631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Detect TPU, GPU, or CPU hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    \n",
        "# Select appropriate distribution strategy\n",
        "if tpu:\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu, steps_per_run=128) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on single GPU  /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Number of accelerators:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcLJyzqHcdwP",
        "colab_type": "text"
      },
      "source": [
        "## Let's Get to (Machine) Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFMlFrTGVC-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the MNIST data from openml\n",
        "X,y = fetch_openml('mnist_784', version=1, cache=True,return_X_y=True) # Pull the mnist dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeSgdqVhXqaL",
        "colab_type": "code",
        "outputId": "782689c9-730a-4393-8d70-b3edde5eb81e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Check distribution before subsampling\n",
        "y_num = y.astype('int64')\n",
        "sns.countplot(y_num)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2a20fd4828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ5LUicEhSerE4JAkdfL/AaPZFRmU4YmJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCkiNBWSozI",
        "colab_type": "text"
      },
      "source": [
        "## Data Subsetting\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eo2rTFRkvTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Subset the data so we get training in some reasonable time for presentation\n",
        "# X_sub, y_sub = X[:70000],y[:70000]\n",
        "\n",
        "# # Check to see that there's no crazy imbalance caused by subsetting\n",
        "# sns.countplot(y_sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXD8GzRI5Kkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Set the subsets to the original variable names\n",
        "# X = X_sub\n",
        "# y = y_sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNO8qgkYJQgB",
        "colab_type": "code",
        "outputId": "6653120b-a404-47ab-c198-ce28f256a7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(f'x shape: {X.shape},\\ny shape: {y.shape}')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x shape: (70000, 784),\n",
            "y shape: (70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8TnMBa7TvWs",
        "colab_type": "text"
      },
      "source": [
        "## Data Transformation\n",
        "\n",
        "Here we rescale the data to the range [0,1] to improve the convergence properties of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQzW0Qa0KHPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the data\n",
        "X_norm = preprocessing.normalize(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slryivLMpwJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the training and test sets\n",
        "y = to_categorical(y,num_classes=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_norm,y,\n",
        "                                                    test_size = 0.25,\n",
        "                                                    random_state=1775)\n",
        "assert(X_train.shape[0]+X_test.shape[0] == X.shape[0]) # Checks that the split was successful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pByHC_FqhgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the 1D vectors (length 784) from mnist into matricies (tensor)\n",
        "X_train = X_train.reshape(-1,28,28,1) \n",
        "X_test = X_test.reshape(-1,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOL75e7lNdK-",
        "colab_type": "code",
        "outputId": "040731e5-432b-481d-ba16-313747a1083c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Check the image plot of the stored training data\n",
        "g = plt.imshow(X_train[0][:,:,0])\n",
        "y_train[0]"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvxAUIQfCIrwA0ERfiCo/welLqo0Op7XOwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1_om6n4cnRS",
        "colab_type": "text"
      },
      "source": [
        "## Keras Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn9bCZ2eRV6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model framework\n",
        "# Model Framework Source: https://www.kaggle.com/wrecked22/digit-recognizer-99-accuracy-using-cnn\n",
        "model = None # Clear model object\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32,\n",
        "                 kernel_size=(5,5), \n",
        "                 padding = 'Same',\n",
        "                 activation='relu',\n",
        "                 input_shape=(28,28,1)))\n",
        "model.add(Conv2D(filters=32,\n",
        "                 kernel_size=(5,5),\n",
        "                 padding='Same',\n",
        "                 activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Conv2D(filters=64,\n",
        "                 kernel_size=(3,3),\n",
        "                 padding = 'Same',\n",
        "                 activation='relu'))\n",
        "model.add(Conv2D(filters=64,\n",
        "                 kernel_size=(3,3),\n",
        "                 padding='Same',\n",
        "                 activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2),\n",
        "                    strides=(2,2)))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41tHZdpoh5pC",
        "colab_type": "code",
        "outputId": "8950fb43-951a-4487-9277-48868040c3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "source": [
        "model.summary() # Check that the model is defined correctly"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 28, 28, 32)        25632     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 256)               803072    \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 887,530\n",
            "Trainable params: 887,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIouKkBj0OFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=0.001,\n",
        "                    rho=0.9,\n",
        "                    epsilon=1e-08,\n",
        "                    decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAFAnsbe1bQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model compilation\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlxnE5AU2MAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set number of epochs to run, generally more epochs gives higher accuracy\n",
        "epochs = 5 # Number of passes through the complete dataset\n",
        "batch_size = 64 # Number of training data to iterate through before updating params\n",
        "\n",
        "# Set a learning rate annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFLunEtoGYK4",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CagvNDot2hBg",
        "colab_type": "code",
        "outputId": "1bb1b6e6-a5e2-4b9c-8893-ce4ee3765f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# model fitting\n",
        "history =  model.fit(X_train,\n",
        "                     y_train,\n",
        "                     batch_size=batch_size,\n",
        "                     epochs=epochs,\n",
        "                     validation_data = (X_test,y_test),verbose=2, \n",
        "                     callbacks=[learning_rate_reduction,TQDMNotebookCallback()])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 52500 samples, validate on 17500 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a585e95ea49949b28a5ee4bfab450763",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=5, style=ProgressStyle(description_width='init…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cf8abf46774448f898f681062472e52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 0', max=52500, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " - 8s - loss: 0.2194 - acc: 0.9293 - val_loss: 0.0598 - val_acc: 0.9819\n",
            "Epoch 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eda35810cbdc4a65a10a9107b6f5ddcc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 1', max=52500, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " - 8s - loss: 0.0719 - acc: 0.9788 - val_loss: 0.0392 - val_acc: 0.9876\n",
            "Epoch 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea8997ac50cb41babe0294e78814366f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 2', max=52500, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " - 8s - loss: 0.0555 - acc: 0.9835 - val_loss: 0.0327 - val_acc: 0.9898\n",
            "Epoch 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39330dc0e84c4a618153ba36734515e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 3', max=52500, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " - 8s - loss: 0.0478 - acc: 0.9864 - val_loss: 0.0330 - val_acc: 0.9901\n",
            "Epoch 5/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab0e5e40bdcd4a5dbe48f9960d655646",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch 4', max=52500, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " - 8s - loss: 0.0422 - acc: 0.9878 - val_loss: 0.0275 - val_acc: 0.9919\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7XGFC53a7td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "image_list = []\n",
        "int_list = []\n",
        "for filename in glob.glob('/content/*.png'): #assuming gif\n",
        "    im=Image.open(filename).convert('L')\n",
        "    tail_string = filename.strip('/content/img-')\n",
        "    numeric_string = int(tail_string.strip('.p'))\n",
        "    int_list.append(numeric_string)\n",
        "    image_list.append(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFgsy1p_0cPV",
        "colab_type": "code",
        "outputId": "acc51537-0ab6-46f1-8541-c5c05b5bacd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "for counter,img in enumerate(image_list):\n",
        "  display((img))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtUlEQVR4nNWQMQ7CMAxFnYKYOAAH\nYOSSETviGqzsLIgFcQImJIQQLLDQQp30f4YGVSruDn+x5afv/FjkT+UvqvnWACcQAAmEfTPt12Xk\nxAkp4tywgVldnEgodlk2vZbHr7UBMB5LTjorYoKBTlXP3vzGJpIk+Vpb9BAAkoQuTbf4cwSAp71c\nxN9BPjqgyIyMrbSNJoZhVQQtS9UIsvgM021lPOiljvm87VzkIVZVjOHWlfXn9Qbr92E/GUcBvgAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A2100B630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiUlEQVR4nNWQIQ7CUBAFXxsqkBju\n23MABsMRkMVyBEgw9SAKCflL/w4CUVKyDgFPbXYym5eV/jb1YReyleN1wNYGdAE8PRz69005jNt2\nYSriRokcmJ/5GqSIYZKiL0gtnENzJlUhdJiaWb4245v7e87gAP1RkjQZ4LwqX7a4bcbmsrNkl7jt\nD+QJy4U97HbdswYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A216B6438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nL2STUsDQRBEK8GT/3oE\n8RwwIWA8eVJQEHMQ/ElqSLIfbmZ36nlYFmFnBU/WZYp+dFM9tPRHPZfxM0ySddklg9NHzi9PBtvA\ncczuop32QdsavBzB0n7p3cYet0anwXacBjvvn9lsKDzO+Rp17kwZJGlpfDWCoYFC0nULh2yVG0MZ\nDsbVxD/ctgZgmyNJqw7IwgxqANxcTKBQ4bhPJj7lcAcxaFWD80QdrCWpgvcMJtp+PMShdvaDkSSd\no25ibCdJi+j2LYO1KcJDBX7N025a2zZUOZMWjY3TbvrEdF/8cnz/qW/MT7AdEgoalAAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A65817DD8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAcUlEQVR4nM1RQQ6AIAyrxgsxvoQP\n8P/TPsBLejXEA7gQmEQuxl5o2NotHfBTCKX/3JR53iyGQtbyhiMW5txgANVBlSa6oiBafZ3rnO2T\nq5BkSqTm0Sj3JQHwVlyQk9nDKtarTm37OoBWOSWsjo2R7jtcxEEpF1rh19wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A21DD1278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAp0lEQVR4nM2QOw7CMBBEJ5QIxFW5\nguuUETSII1DQQEGPqLkBDV2ABEfB+cxQgaI47rPVep+ed7TAmMs4dwnCTLx235NOH8+BfUj8kLsQ\nO5JFMA1FE4K59OiN/oHMDEpCohVP/dnPjKdyKOoqHVr7JLe3RhJfxjMjIDu7hooWS89ck28AOEi1\n/28p3QFjRevDRFLbUqw3A5FySiLL1QADTFpVNnj58dcXX5RcJD25ha4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A65817BA8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApklEQVR4nN3PsQrCQAwG4B+pDy7d\npHQRdOnmYBHcXFzE13BzcNZq2wPvvP93ORDu2gfQLAn5SEiAn4zTeTZGB0tS/jpk6zclSdoPoJXu\nF0OJJl0uCQBWTurGECXFIkYfELWXidHJh6qjryI0YjhkTj4iPFJNKKlX8gu5BABsyFuMO4pbABXF\nPHmml+iso9R+m5OQi1bIphnUl8kggLyxzj4XQ/QH8QHKnGymyrkwXgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A65817080>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAwElEQVR4nNXRvU4CYRSE4cEb8Uq8\nuLXQwlgRSSgouAVjbyyMLXQUVrYGkw3sLt/PzmshBbB8PU77ZHLOyZH+X6rGdp82T+dwDQD2phri\n1oQQs+04GeDz11KSpjvwtDi8hfRQ3gx35cVb+6WIlR3K1Rp/DBp1bzunZmW3p9ixj4HZsb0aYowp\nG6Bv7g8xw60k6Q3A+VOSrvY4kq4laX4j3h+7bnHY/Ab3KWXDbnjBLPnvMc2Zx2hchxDCz13p/kvI\nL3HjkGzdw+rcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A6519F438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmUlEQVR4nNXPrQ7CMBSG4W9zXPYc\nAoPAEQwYFAIUN7MbAAJLmqWsPS+GJYy1cwg+1Zwn56fS73OzsMpZBbgceiBk+loMYhod+IB9lsr+\nsZyJ+QNtE30XozuoMpqxnYG9pIhPLsSdJhCzODz3fdDiHo2iKKXSH1N/qa7PDgh1CiUpQLPJYcdz\nvLMPKvI4zBe62OZxXe+mRv1vXvW+Xa73IluAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A6519F940>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnklEQVR4nN2QOwoCUQxFL4JWrsN9\n2motFpY27kBBBBmwdwWCYGPjB5yPkzc5ForI+GYBequQk+QmkX5Os9RCMDv1Y3AfAMDT4TdcZ1aW\nVjn4vGn6wiEkDfAAkEXRKgfg+pFqvaNeR5K0jXYmebhX7jZtWmmQgUevlSTd4BjxlCSN21K3Vr8s\nKn8J29Tg7vU/3ItJ3Wl0KZ46R577/3oAaU1mzn6WoFkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A6519F908>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmElEQVR4nM3PMQrCQBCF4WfiKbzt\nXkKsLETwDnYWHsLOQpEUARFN4urObyFCIrN9pnvzMfBGGvGE+tVsc9iAWbdxbWVgYOe9g63ZPCQj\nPRxMRGl5hOjiU9IJqt+m6HMpaSYtnMsbXEMHeG3Dty0X95cKAHNNCnXE7hmUamydxSfvXiqGOJHl\nsRykP4yp7aXpEA/FLttn7PMBIyNUzMPPFcIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F2A2102B550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbF61pEi1Ug6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_list = []\n",
        "for i in image_list:\n",
        "  pred = model.predict(preprocessing.normalize((np.array(i))).reshape(-1,28,28,1))\n",
        "  predict_list.append(pred.argmax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYKWVZNx3qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "099c3c37-b42e-4f72-9515-97b75975f127"
      },
      "source": [
        "print('Actual:   ',int_list)\n",
        "print('Predicted:',predict_list)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual:    [5, 1, 8, 7, 6, 0, 2, 4, 3, 9]\n",
            "Predicted: [5, 1, 8, 7, 6, 9, 2, 8, 3, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHntd4FsBBvX",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "* Deep Learning - Goodfellow-et-al-2016\n",
        "\n",
        "* http://cs231n.github.io/convolutional-networks/\n",
        "\n",
        "* https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
        "\n",
        "* Code: https://www.kaggle.com/wrecked22/digit-recognizer-99-accuracy-using-cnn\n"
      ]
    }
  ]
}